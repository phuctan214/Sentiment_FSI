{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_Tokenizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3YFtuBKP57qGncr/4MXsj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Refj_HFhMUtD","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1602751090611,"user_tz":-420,"elapsed":6454,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"d80d9788-cc21-416a-e9c9-f196262c684e"},"source":["!pip install tokenizers\n","!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.91\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jf2KnWJzTo2-","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602749786554,"user_tz":-420,"elapsed":2158,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"82dfa5d9-b18a-4f12-a89c-ac4c1108f4c3"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive') "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eCHom99BTxzu","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1602753515292,"user_tz":-420,"elapsed":2062,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"008789f1-5f3e-42bf-f6b5-fe5aed8c65af"},"source":["%cd '/content/drive/My Drive/Da Poet'\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Da Poet\n","big.txt  merges.txt  Train_Tokenizer.ipynb  VNTQcorpus-small.txt  vocab.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ke5LOhCQSdYw"},"source":["#BIG_FILE_URL = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n","\n","# Let's download the file and save it somewhere\n","#from requests import get\n","with open('VNTQcorpus-small.txt', 'wb') as big_f:\n","\n","  #response = get(BIG_FILE_URL, )\n","  big_f.write(VNTQcorpus-small.txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g51Ai-ydSet_"},"source":["# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n","# the overall pipeline for various well-known tokenization algorithm. \n","# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n","\n","from tokenizers import Tokenizer\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.models import BPE\n","from tokenizers.normalizers import Lowercase, NFKC, Sequence\n","from tokenizers.pre_tokenizers import ByteLevel\n","import sentencepiece as spm \n","# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n","tokenizer = Tokenizer(BPE())\n","\n","# Then we enable lower-casing and unicode-normalization\n","# The Sequence normalizer allows us to combine multiple Normalizer that will be\n","# executed in order.\n","tokenizer.normalizer = Sequence([\n","    NFKC(),\n","    Lowercase()\n","])\n","\n","# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n","tokenizer.pre_tokenizer = ByteLevel()\n","\n","# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n","tokenizer.decoder = ByteLevelDecoder()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Apci6IFShUy","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602753579837,"user_tz":-420,"elapsed":16174,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"f3bb719f-e8f6-4de6-cfd9-fb722039296b"},"source":["from tokenizers.trainers import BpeTrainer\n","\n","trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n","tokenizer.train(trainer, [\"VNTQcorpus-small.txt\"])\n","\n","print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trained vocab size: 18434\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nJ-nKigoSlIi","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602753689224,"user_tz":-420,"elapsed":1630,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"ad744da3-6a34-4624-95e9-259f55c0ff04"},"source":["tokenizer.model.save('.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./vocab.json', './merges.txt']"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"UbtoHfDES2-U","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1602753736012,"user_tz":-420,"elapsed":857,"user":{"displayName":"tân Phúc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDKgBtrP3HG_eM7k1pazVmgUON4-FhFdANHuo1=s64","userId":"07473053315684123316"}},"outputId":"d4c0c00b-ef35-4f58-ab97-2d5d7c747929"},"source":["tokenizer.model = BPE('vocab.json', 'merges.txt')\n","encoding = tokenizer.encode(\"hôm nay trời mưa\")\n","\n","print(encoding.ids)\n","decoded = tokenizer.decode(encoding.ids)\n","print(decoded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1086, 757, 845, 1688]\n"," hôm nay trời mưa\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]}]}